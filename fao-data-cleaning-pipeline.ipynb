{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12908308,"sourceType":"datasetVersion","datasetId":8167711},{"sourceId":258930159,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **FAO Data Cleaning Pipeline**\nThis notebook is designed to perform a comprehensive cleaning and transformation process on the FAOSTAT datasets after they have been downloaded. It will read the raw CSV files from the */kaggle/working/ directory*, perform several cleaning steps, and save the resulting tidy data to a new Clean directory.","metadata":{}},{"cell_type":"markdown","source":"## **1. Set Up Paths and Logging**\nFirst, we'll configure the project's directory structure and set up the logging system. On Kaggle, the working directory is /kaggle/working/, so we'll adjust the paths to reflect this.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime\nimport os\n\n# ====================================================================\n# === 1. Path Configuration ===\n# ====================================================================\n# The base path for reading data is the read-only Kaggle input directory.\nBASE_DATA_DIR_PATH = Path(\"/kaggle/input/fao-americas-bulk-data/FAO_Data_Pipeline\")\nRAW_DIR = BASE_DATA_DIR_PATH / \"Raw\"\n\n# The path for writing cleaned data is the writable Kaggle working directory.\nCLEAN_DIR = Path(\"/kaggle/working/FAO_Data_Pipeline/Clean\")\n\n# The path for logs is also in the writable directory.\nLOG_DIR = Path(\"/kaggle/working/FAO_Data_Pipeline/Logs\")\n\n# Create directories for output if they don't exist\nCLEAN_DIR.mkdir(parents=True, exist_ok=True)\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\n\n# ====================================================================\n# === 2. Configure Logging ===\n# ====================================================================\nlog_filename = datetime.now().strftime(\"data_cleaning_log_%Y-%m-%d_%H-%M-%S.log\")\nlog_filepath = LOG_DIR / log_filename\n\nlogging.basicConfig(\n    filename=str(log_filepath),\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T18:39:39.617142Z","iopub.execute_input":"2025-08-29T18:39:39.617741Z","iopub.status.idle":"2025-08-29T18:39:39.626643Z","shell.execute_reply.started":"2025-08-29T18:39:39.617698Z","shell.execute_reply":"2025-08-29T18:39:39.625353Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **2. Define Cleaning Rules and Country Codes**\nThis cell contains the dictionaries and lists that define the cleaning logic, including the specific columns to keep for each dataset and the FAO area codes for countries in the Americas.","metadata":{}},{"cell_type":"code","source":"# ====================================================================\n# === 3. Define Columns and Country Codes by Region ===\n# ====================================================================\n# This dictionary maps each file to the columns that must be kept\ndataset_columns = {\n    \"Consumer_Price_Indices\": [\"Area Code\", \"Area\", \"Months\", \"Element\", \"Unit\"],\n    \"Credit_to_Agriculture\": [\"Area Code\", \"Area\", \"Item\", \"Element\", \"Unit\"],\n    \"Crops_and_Livestock_Products_Indicators\": [\"Area Code\", \"Area\", \"Indicator\", \"Unit\"],\n    \"Emissions_Indicators\": [\"Area Code\", \"Area\", \"Item\", \"Element\", \"Unit\"],\n    \"Emissions_totals\": [\"Area Code\", \"Area\", \"Item\", \"Element\", \"Unit\"],\n    \"Fertilizers_by_Product\": [\"Area Code\", \"Area\", \"Item\", \"Element\", \"Unit\"],\n    \"Land_Use\": [\"Area Code\", \"Area\", \"Item\", \"Element\", \"Unit\"],\n    \"Macro_Indicators\": [\"Area Code\", \"Area\", \"Element\", \"Unit\"],\n    \"Pesticide_Use\": [\"Area Code\", \"Area\", \"Item\", \"Element\", \"Unit\"],\n    \"Producer_Prices\": [\"Area Code\", \"Area\", \"Item\", \"Element\", \"Unit\"],\n    \"Production_Indices\": [\"Area Code\", \"Area\", \"Item\", \"Element\", \"Unit\"],\n    \"Value_of_Agricultural_Production\": [\"Area Code\", \"Area\", \"Item\", \"Element\", \"Unit\"],\n    \"Value_Shares_by_Industry_and_Primary_Factors\": [\"Area Code\", \"Area\", \"Industry\", \"Factor\", \"Element\", \"Unit\"],\n}\n\n# FAO area codes for each region of the Americas\ncaribbean_codes = [\n    8, 22, 12, 14, 36, 49, 55, 56, 86, 87, 93, 109, 135, 177, 188, 189, 190, 191, 220, 224, 239, 240, 258, 142, 151\n]\ncentral_north_america_codes = [\n    23, 48, 60, 89, 95, 138, 157, 166, 33, 231\n]\nsouth_america_codes = [\n    9, 19, 21, 40, 44, 58, 69, 91, 169, 170, 207, 234, 236\n]\n\n# Mapping of area codes to their regions for easy assignment\nregion_mapping = {}\nfor code in caribbean_codes:\n    region_mapping[code] = \"Caribbean\"\nfor code in central_north_america_codes:\n    region_mapping[code] = \"CentralNorthAmerica\"\nfor code in south_america_codes:\n    region_mapping[code] = \"SouthAmerica\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T18:39:59.330902Z","iopub.execute_input":"2025-08-29T18:39:59.331328Z","iopub.status.idle":"2025-08-29T18:39:59.341075Z","shell.execute_reply.started":"2025-08-29T18:39:59.331303Z","shell.execute_reply":"2025-08-29T18:39:59.339923Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Implement the Core Cleaning Function** \n\nThis function, clean_fao_data, contains the main logic for reading, transforming, and saving each dataset. It handles various tasks, including reshaping the data, dropping null values, and adding the region column.","metadata":{}},{"cell_type":"code","source":"# ====================================================================\n# === 4. Data Cleaning Function ===\n# ====================================================================\ndef clean_fao_data(file_path):\n    \"\"\"\n    Reads a CSV file, selects the specified columns,\n    converts from 'wide' to 'long' format, removes null records,\n    and adds a region column before saving.\n    \"\"\"\n    file_name = file_path.stem  # Get the file name without extension\n    logging.info(f\"Processing file: {file_name}\")\n\n    if file_name not in dataset_columns:\n        logging.warning(f\"No column configuration found for {file_name}. Skipping.\")\n        return\n\n    try:\n        try:\n            df = pd.read_csv(file_path, encoding=\"utf-8\")\n        except UnicodeDecodeError:\n            logging.warning(\n                f\"Failed to read {file_name} with utf-8. Trying with latin1 encoding.\"\n            )\n            df = pd.read_csv(file_path, encoding=\"latin1\")\n\n        keep_columns = dataset_columns[file_name]\n        year_columns = [col for col in df.columns if col.startswith(\"Y\")]\n        existing_keep_columns = [col for col in keep_columns if col in df.columns]\n        missing_columns = [col for col in keep_columns if col not in df.columns]\n\n        if missing_columns:\n            logging.warning(\n                f\"Missing columns for {file_name}: {missing_columns}. Skipping this file.\"\n            )\n            return\n\n        id_vars = existing_keep_columns\n        value_vars = year_columns\n\n        if not value_vars:\n            logging.warning(\n                f\"No year columns found for {file_name}. Skipping this file.\"\n            )\n            return\n\n        df_melted = pd.melt(\n            df,\n            id_vars=id_vars,\n            value_vars=value_vars,\n            var_name=\"Year\",\n            value_name=\"Value\",\n        )\n\n        initial_rows = len(df_melted)\n        df_melted.dropna(subset=[\"Value\"], inplace=True)\n        dropped_rows_nulls = initial_rows - len(df_melted)\n\n        logging.info(\n            f\"Dropped {dropped_rows_nulls} records with null values in 'Value' column from {file_name}\"\n        )\n        print(\n            f\"Removed {dropped_rows_nulls} empty 'Value' records from {file_name}.csv\"\n        )\n\n        df_melted[\"Year\"] = df_melted[\"Year\"].str.replace(\"Y\", \"\").astype(int)\n\n        all_americas_codes = (\n            caribbean_codes + central_north_america_codes + south_america_codes\n        )\n        initial_rows_before_filter = len(df_melted)\n        df_filtered = df_melted[df_melted[\"Area Code\"].isin(all_americas_codes)]\n        dropped_rows_filter = initial_rows_before_filter - len(df_filtered)\n\n        logging.info(\n            f\"Dropped {dropped_rows_filter} records not belonging to Americas from {file_name}\"\n        )\n        print(\n            f\"Removed {dropped_rows_filter} records not in Americas from {file_name}.csv\"\n        )\n\n        df_filtered[\"Region\"] = df_filtered[\"Area Code\"].map(region_mapping)\n        df_filtered.rename(columns={\"Area\": \"Country\"}, inplace=True)\n\n        cleaned_file_path = CLEAN_DIR / f\"{file_name}_cleaned.csv\"\n        df_filtered.to_csv(cleaned_file_path, index=False)\n\n        logging.info(f\"Successfully cleaned and saved: {cleaned_file_path}\")\n        print(f\"Cleaned {file_name}.csv and saved to {cleaned_file_path}\")\n\n    except Exception as e:\n        logging.error(f\"Error cleaning {file_name}: {e}\")\n        print(f\"Error cleaning {file_name}: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T18:40:18.414729Z","iopub.execute_input":"2025-08-29T18:40:18.415036Z","iopub.status.idle":"2025-08-29T18:40:18.428741Z","shell.execute_reply.started":"2025-08-29T18:40:18.415014Z","shell.execute_reply":"2025-08-29T18:40:18.427602Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Run the Data Cleaning Pipeline**\nThis is the final step. Executing this cell will call the main function, run_fao_cleaning_pipeline, which will iterate through the files in the raw data directory and apply the cleaning function to each one.","metadata":{}},{"cell_type":"code","source":"# ====================================================================\n# === 5. Run the Cleaning Pipeline ===\n# ====================================================================\ndef run_fao_cleaning_pipeline():\n    \"\"\"\n    Main function to run the cleaning pipeline on all\n    files in the 'Raw' folder.\n    \"\"\"\n    logging.info(\"Starting FAO data cleaning pipeline\")\n\n    raw_files = list(RAW_DIR.glob(\"*.csv\"))\n\n    if not raw_files:\n        logging.warning(\n            \"No CSV files found in the 'Raw' directory. Please ensure the data has been added to this notebook.\"\n        )\n        print(\n            \"No CSV files found in the 'Raw' directory. Please ensure the data has been added to this notebook.\"\n        )\n        return\n\n    for file in raw_files:\n        clean_fao_data(file)\n\n    logging.info(\"FAO data cleaning pipeline finished\")\n    print(\"All files processed.\")\n\n\nif __name__ == \"__main__\":\n    run_fao_cleaning_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T18:40:30.288806Z","iopub.execute_input":"2025-08-29T18:40:30.289174Z","iopub.status.idle":"2025-08-29T18:40:42.815826Z","shell.execute_reply.started":"2025-08-29T18:40:30.289147Z","shell.execute_reply":"2025-08-29T18:40:42.814886Z"}},"outputs":[],"execution_count":null}]}